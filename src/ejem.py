# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iJLnomeMzLjOTiXFm7qRQ1Bse-60hUQq
"""

# --- 1. Preparaci√≥n del Entorno ---
# Instalar las librer√≠as necesarias
# transformers para trabajar con modelos de lenguaje de Hugging Face
# datasets para manejar colecciones de datos f√°cilmente
print("Aseg√∫rate de tener instaladas las librer√≠as: transformers, scikit-learn, datasets, pandas.")
print("Librer√≠as instaladas.")

# Verificar si estamos usando una GPU (recomendado para el entrenamiento)

# Suprimir warnings de PyTorch y transformers
import warnings
warnings.filterwarnings("ignore")
import os
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "1"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

import torch
if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"Usando GPU: {torch.cuda.get_device_name(0)}")
else:
    device = torch.device("cpu")
    print("No se encontr√≥ GPU, usando CPU. El entrenamiento ser√° m√°s lento.")

print("\n--- 2. Carga y Preparaci√≥n del Dataset Divertido: Clasificaci√≥n de Emojis ---")

# Definir un dataset muy peque√±o y divertido de frases y sus emojis
# Esto simula un escenario donde tenemos datos espec√≠ficos y limitados
data = {
    "text": [
        "Me siento muy feliz hoy!",
        "Estoy bastante triste por las noticias.",
        "Esto es incre√≠ble! Me encanta!",
        "Qu√© d√≠a tan aburrido...",
        "Estoy emocionado por el bootcamp!",
        "No me gusta nada esto.",
        "Esto es una maravilla",
        "Me quiero ir a dormir",
        "Gracias por tu ayuda",
        "Que bien",
        "Lo siento mucho"
    ],
    "label": [
        "üòä",  # Feliz
        "üòî",  # Triste
        "ü§©",  # Asombro/Admiraci√≥n
        "üòê",  # Aburrido/Neutral
        "ü•≥",  # Fiesta/Emoci√≥n
        "üò†",  # Enfadado
        "ü§©",
        "üò¥",
        "üôè",
        "üòä",
        "üòî"
    ]
}

# Mapear los emojis a IDs num√©ricos para que el modelo los entienda
# Y viceversa para poder interpretar las predicciones
unique_labels = list(set(data["label"]))
label_to_id = {label: i for i, label in enumerate(unique_labels)}
id_to_label = {i: label for i, label in enumerate(unique_labels)}

print(f"Emojis √∫nicos detectados: {unique_labels}")
print(f"Mapeo de etiquetas a IDs: {label_to_id}")

# Convertir el dataset a un formato compatible con la librer√≠a datasets de Hugging Face
from datasets import Dataset, Features, Value, ClassLabel
import pandas as pd

df = pd.DataFrame(data)
df['label_id'] = df['label'].map(label_to_id)

# Definir las caracter√≠sticas del dataset, incluyendo las etiquetas como ClassLabel
features = Features({
    'text': Value(dtype='string'),
    'label': Value(dtype='string'), # Mantener la columna original si es necesario para referencia
    'label_id': ClassLabel(names=unique_labels)
})

dataset = Dataset.from_pandas(df, features=features)

print("\nDataset original (primeras filas):")
print(dataset.to_pandas().head())

# Cargar el tokenizador de DistilBERT
# El tokenizador convierte el texto en IDs num√©ricos que el modelo puede procesar
from transformers import AutoTokenizer
model_name = "distilbert-base-uncased" # Un SLM popular, m√°s peque√±o que BERT

print(f"\nCargando tokenizador para el modelo: {model_name}...")
tokenizer = AutoTokenizer.from_pretrained(model_name)
print("Tokenizador cargado.")

# Funci√≥n para tokenizar el dataset
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=50) # max_length para eficiencia

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Dividir el dataset en conjuntos de entrenamiento y validaci√≥n
# Usamos un 80% para entrenar y un 20% para validar el modelo
print("\nDividiendo el dataset en entrenamiento y validaci√≥n (80/20)...")
train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42) # Usamos una semilla para reproducibilidad
train_dataset = train_test_split["train"]
eval_dataset = train_test_split["test"]

print(f"Tama√±o del dataset de entrenamiento: {len(train_dataset)} ejemplos")
print(f"Tama√±o del dataset de validaci√≥n: {len(eval_dataset)} ejemplos")

# Preparar el formato para el entrenamiento
train_dataset = train_dataset.remove_columns(["text", "label"]) # Eliminamos las columnas de texto y emoji original
eval_dataset = eval_dataset.remove_columns(["text", "label"])
train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label_id"])
eval_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label_id"])

# Renombrar la columna de etiquetas para que sea 'labels' (requerido por el Trainer de Hugging Face)
train_dataset = train_dataset.rename_column("label_id", "labels")
eval_dataset = eval_dataset.rename_column("label_id", "labels")

print("\n--- 3. Carga del Modelo SLM (DistilBERT) ---")

# Cargar el modelo DistilBERT pre-entrenado para clasificaci√≥n de secuencias
# num_labels es el n√∫mero de clases (emojis) que queremos clasificar
from transformers import AutoModelForSequenceClassification

print(f"Cargando el modelo pre-entrenado: {model_name} para {len(unique_labels)} etiquetas...")
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=len(unique_labels),
    id2label=id_to_label, # Para que el modelo sepa el nombre de cada label
    label2id=label_to_id # Y viceversa
).to(device) # Mover el modelo a la GPU si est√° disponible
print("Modelo cargado y configurado para clasificaci√≥n.")

print("\n--- 4. Fine-tuning del SLM (¬°Entrenamiento r√°pido!) ---")

# Importar las clases necesarias para el entrenamiento
from transformers import TrainingArguments, Trainer
from sklearn.metrics import accuracy_score
import numpy as np

# Funci√≥n para calcular las m√©tricas de evaluaci√≥n (en este caso, la precisi√≥n)
def compute_metrics(p):
    predictions = np.argmax(p.predictions, axis=1)
    return {"accuracy": accuracy_score(p.label_ids, predictions)}

# Configurar los argumentos de entrenamiento
# Aqu√≠ definimos c√≥mo se comportar√° el proceso de entrenamiento (√©pocas, tasa de aprendizaje, etc.)
training_args = TrainingArguments(
    output_dir="./results",                     # Directorio donde se guardar√°n los resultados
    eval_strategy="epoch",                # Evaluar despu√©s de cada √©poca
    learning_rate=2e-5,                         # Tasa de aprendizaje
    per_device_train_batch_size=8,             # Tama√±o del batch por dispositivo para entrenamiento
    per_device_eval_batch_size=8,              # Tama√±o del batch por dispositivo para evaluaci√≥n
    num_train_epochs=5,                         # N√∫mero de √©pocas de entrenamiento (ajustar seg√∫n necesidad)
    weight_decay=0.01,                          # Regularizaci√≥n para evitar el sobreajuste
    logging_dir='./logs',                       # Directorio para los logs de TensorBoard
    logging_steps=10,
    report_to="none"                            # Deshabilitar reportes a W&B u otros
)

# Crear el objeto Trainer
# El Trainer se encarga de todo el bucle de entrenamiento, evaluaci√≥n y guardado
trainer = Trainer(
    model=model,                                # El modelo a entrenar
    args=training_args,                         # Los argumentos de entrenamiento
    train_dataset=train_dataset,                # El dataset de entrenamiento
    eval_dataset=eval_dataset,                  # El dataset de validaci√≥n
    tokenizer=tokenizer,                        # El tokenizador
    compute_metrics=compute_metrics             # La funci√≥n para calcular m√©tricas
)

print("\nComenzando el fine-tuning del modelo. ¬°Esto ser√° r√°pido!")
# ¬°Entrenar el modelo!
trainer.train()
print("\nFine-tuning completado.")

print("\n--- 5. Evaluaci√≥n y Predicciones ---")

# Evaluar el modelo en el conjunto de validaci√≥n
print("\nEvaluando el modelo en el conjunto de validaci√≥n...")
results = trainer.evaluate()
print(f"Resultados de la evaluaci√≥n: {results}")

# Probar el modelo con nuevas frases (¬°Momento interactivo!)
def predict_emoji(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=50).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    # Obtener la clase con la mayor probabilidad
    predicted_class_id = torch.argmax(logits, dim=1).item()
    # Convertir el ID de la clase de nuevo a su emoji
    predicted_emoji = id_to_label[predicted_class_id]
    return predicted_emoji

print("\n¬°Hora de probar el modelo! Ingresa frases y el SLM intentar√° predecir el emoji.")
print("Escribe 'salir' para terminar.")

while True:
    user_input = input("\nIngresa una frase (o 'salir' para terminar): ")
    if user_input.lower() == 'salir':
        break
    predicted = predict_emoji(user_input)
    print(f"El SLM predice: {predicted}")

print("\n¬°Gracias por probar el modelo de clasificaci√≥n de emojis con un SLM!")