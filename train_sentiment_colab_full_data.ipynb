{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdomdev/ai-small-language-model/blob/feature%2Fimdb-sentiment/train_sentiment_colab_full_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f1eafad",
      "metadata": {
        "id": "4f1eafad"
      },
      "source": [
        "El modelo actual pesa 257MB, lo que confirma que la división será necesaria para subirlo a GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "289eab4f",
      "metadata": {
        "id": "289eab4f"
      },
      "source": [
        "Vamos a crear el notebook de Colab paso a paso. Te proporcionaré el contenido del notebook en bloques de código que podrás copiar\n",
        "y pegar en un nuevo archivo .ipynb en Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8166026d",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "8166026d"
      },
      "source": [
        "Contenido del Notebook de Colab (train_sentiment_colab.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89068404",
      "metadata": {
        "id": "89068404"
      },
      "source": [
        "Bloque 1: Configuración Inicial e Importaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18f5e8e7",
      "metadata": {
        "id": "18f5e8e7"
      },
      "source": [
        "Este bloque se encarga de instalar las librerías necesarias (aunque muchas ya están en Colab, es buena práctica incluirlas para\n",
        "asegurar la reproducibilidad), montar Google Drive y definir las rutas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da30bd3f",
      "metadata": {
        "id": "da30bd3f"
      },
      "source": [
        "@title 1. Configuración Inicial e Importaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "386b97e9",
      "metadata": {
        "id": "386b97e9"
      },
      "outputs": [],
      "source": [
        "# Instalar librerías necesarias (muchas ya están en Colab, pero es buena práctica)\n",
        "!pip install transformers[torch] datasets evaluate scikit-learn accelerate pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "118b72fd",
      "metadata": {
        "id": "118b72fd"
      },
      "outputs": [],
      "source": [
        "# Importar librerías\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset\n",
        "from evaluate import load\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6be50285",
      "metadata": {
        "id": "6be50285"
      },
      "outputs": [],
      "source": [
        "# Montar Google Drive para guardar/cargar archivos grandes\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc54a1ed",
      "metadata": {
        "id": "fc54a1ed"
      },
      "outputs": [],
      "source": [
        "# Definir la ruta base en Google Drive para guardar los resultados y el modelo\n",
        "# Asegúrate de que esta carpeta exista en tu Google Drive\n",
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive/SLM_Training_Results\"\n",
        "os.makedirs(DRIVE_BASE_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca588b43",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "ca588b43"
      },
      "outputs": [],
      "source": [
        "print(\"Configuración inicial completada.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2340ee00",
      "metadata": {
        "id": "2340ee00"
      },
      "source": [
        "Bloque 2: Carga y Guardado de Datasets Originales a CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cf37230",
      "metadata": {
        "id": "2cf37230"
      },
      "source": [
        "Aquí cargaremos los datasets completos de IMDb y los guardaremos como CSVs separados en tu Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45d87ee8",
      "metadata": {
        "id": "45d87ee8"
      },
      "source": [
        "@title 2. Carga y Guardado de Datasets Originales a CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f7e7c7f",
      "metadata": {
        "id": "0f7e7c7f"
      },
      "outputs": [],
      "source": [
        "print(\"Cargando el dataset IMDb completo...\")\n",
        "dataset = load_dataset(\"imdb\")\n",
        "print(\"Dataset IMDb cargado.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b11441e5",
      "metadata": {
        "id": "b11441e5"
      },
      "outputs": [],
      "source": [
        "# Convertir a Pandas DataFrame y guardar como CSV\n",
        "df_train_original = pd.DataFrame(dataset[\"train\"])\n",
        "df_test_original = pd.DataFrame(dataset[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e98e681c",
      "metadata": {
        "id": "e98e681c"
      },
      "outputs": [],
      "source": [
        "train_csv_path = os.path.join(DRIVE_BASE_PATH, \"imdb_train_original.csv\")\n",
        "test_csv_path = os.path.join(DRIVE_BASE_PATH, \"imdb_test_original.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "530df172",
      "metadata": {
        "id": "530df172"
      },
      "outputs": [],
      "source": [
        "df_train_original.to_csv(train_csv_path, index=False)\n",
        "df_test_original.to_csv(test_csv_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5db00f1",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "b5db00f1"
      },
      "outputs": [],
      "source": [
        "print(f\"Dataset de entrenamiento original guardado en: {train_csv_path}\")\n",
        "print(f\"Dataset de prueba original guardado en: {test_csv_path}\")\n",
        "print(f\"Tamaño del dataset de entrenamiento original: {len(df_train_original)} registros\")\n",
        "print(f\"Tamaño del dataset de prueba original: {len(df_test_original)} registros\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e83ce0d",
      "metadata": {
        "id": "5e83ce0d"
      },
      "source": [
        "Bloque 3: Unificación y Limpieza de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60cffaa9",
      "metadata": {
        "id": "60cffaa9"
      },
      "source": [
        "Este es el bloque crucial para la limpieza y unificación. Incluiré una función de limpieza básica y comprobaciones de\n",
        "nulos/duplicados."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0efc7da5",
      "metadata": {
        "id": "0efc7da5"
      },
      "source": [
        "@title 3. Unificación y Limpieza de Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed8cd9af",
      "metadata": {
        "id": "ed8cd9af"
      },
      "outputs": [],
      "source": [
        "print(\"Unificando datasets de entrenamiento y prueba...\")\n",
        "df_full = pd.concat([df_train_original, df_test_original], ignore_index=True)\n",
        "print(f\"Dataset unificado creado con {len(df_full)} registros.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b06a7fd",
      "metadata": {
        "id": "0b06a7fd"
      },
      "outputs": [],
      "source": [
        "print(\"Realizando comprobaciones de limpieza de datos...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d2fc6a3",
      "metadata": {
        "id": "6d2fc6a3"
      },
      "outputs": [],
      "source": [
        "# Comprobar nulos\n",
        "print(f\"Valores nulos por columna antes de la limpieza:\\n{df_full.isnull().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be79f2c0",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "be79f2c0"
      },
      "outputs": [],
      "source": [
        "# Comprobar duplicados\n",
        "print(f\"Registros duplicados antes de la limpieza: {df_full.duplicated().sum()}\")\n",
        "if df_full.duplicated().sum() > 0:\n",
        "    df_full.drop_duplicates(inplace=True)\n",
        "    print(f\"Registros duplicados eliminados. Nuevo tamaño: {len(df_full)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c7330ac",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "1c7330ac"
      },
      "outputs": [],
      "source": [
        "# Función de limpieza de texto\n",
        "def clean_text(text):\n",
        "    text = str(text).lower() # Convertir a string y a minúsculas\n",
        "    text = re.sub(r'<br />', ' ', text) # Eliminar etiquetas <br />\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text) # Eliminar caracteres especiales (mantener letras, números, espacios)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Eliminar espacios extra\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa5b28a2",
      "metadata": {
        "id": "aa5b28a2"
      },
      "outputs": [],
      "source": [
        "print(\"Aplicando limpieza de texto a la columna 'text'...\")\n",
        "df_full['text'] = df_full['text'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a490ecbc",
      "metadata": {
        "id": "a490ecbc"
      },
      "outputs": [],
      "source": [
        "# Comprobar si hay \"nulos\" como cadenas vacías o solo espacios después de la limpieza\n",
        "print(f\"Registros con texto vacío después de la limpieza: {(df_full['text'] == '').sum()}\")\n",
        "if (df_full['text'] == '').sum() > 0:\n",
        "    df_full = df_full[df_full['text'] != '']\n",
        "    print(f\"Registros con texto vacío eliminados. Nuevo tamaño: {len(df_full)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d84f5d9",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "5d84f5d9"
      },
      "outputs": [],
      "source": [
        "# Guardar el dataset unificado y limpio\n",
        "full_cleaned_csv_path = os.path.join(DRIVE_BASE_PATH, \"imdb_full_cleaned.csv\")\n",
        "df_full.to_csv(full_cleaned_csv_path, index=False)\n",
        "print(f\"Dataset unificado y limpio guardado en: {full_cleaned_csv_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "792042b4",
      "metadata": {
        "id": "792042b4"
      },
      "source": [
        "Bloque 4: División 80/20 para Entrenamiento y Prueba"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5467ef3e",
      "metadata": {
        "id": "5467ef3e"
      },
      "source": [
        "Aquí dividiremos el dataset limpio en 80% para entrenamiento y 20% para prueba."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce208d1a",
      "metadata": {
        "id": "ce208d1a"
      },
      "source": [
        "@title 4. División 80/20 para Entrenamiento y Prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe7fbf00",
      "metadata": {
        "id": "fe7fbf00"
      },
      "outputs": [],
      "source": [
        "print(\"Dividiendo el dataset unificado en 80% entrenamiento y 20% prueba...\")\n",
        "train_df, test_df = train_test_split(df_full, test_size=0.2, random_state=42, stratify=df_full['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3e8be62",
      "metadata": {
        "id": "a3e8be62"
      },
      "outputs": [],
      "source": [
        "# Convertir DataFrames de Pandas a objetos Dataset de Hugging Face\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23380d61",
      "metadata": {
        "id": "23380d61"
      },
      "outputs": [],
      "source": [
        "# Eliminar la columna '__index_level_0__' que se añade automáticamente al convertir de pandas\n",
        "train_dataset = train_dataset.remove_columns([\"__index_level_0__\"])\n",
        "test_dataset = test_dataset.remove_columns([\"__index_level_0__\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dab6be7",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "0dab6be7"
      },
      "outputs": [],
      "source": [
        "print(f\"Tamaño del dataset de entrenamiento (80%): {len(train_dataset)} registros\")\n",
        "print(f\"Tamaño del dataset de prueba (20%): {len(test_dataset)} registros\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "117cc493",
      "metadata": {
        "id": "117cc493"
      },
      "source": [
        "Bloque 5: Tokenización y Carga del Modelo (Adaptado del Script Original)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76c91103",
      "metadata": {
        "id": "76c91103"
      },
      "source": [
        "Este bloque es una adaptación directa de tu train_sentiment_model.py."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47783b2f",
      "metadata": {
        "id": "47783b2f"
      },
      "source": [
        "@title 5. Tokenización y Carga del Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b4810d0",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "5b4810d0"
      },
      "outputs": [],
      "source": [
        "# Cargar el Tokenizador\n",
        "print(\"\\nCargando el tokenizador DistilBERT...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ec786b5",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "2ec786b5"
      },
      "outputs": [],
      "source": [
        "# Función de Preprocesamiento\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bedcfc83",
      "metadata": {
        "id": "bedcfc83"
      },
      "outputs": [],
      "source": [
        "print(\"\\nPreprocesando el dataset de entrenamiento y prueba...\")\n",
        "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef234c21",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "ef234c21"
      },
      "outputs": [],
      "source": [
        "# Cargar el Modelo\n",
        "print(\"\\nCargando el modelo DistilBERT para clasificación de secuencias...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15681754",
      "metadata": {
        "id": "15681754"
      },
      "source": [
        "Bloque 6: Definición de Métricas y Configuración de Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82995b4f",
      "metadata": {
        "id": "82995b4f"
      },
      "source": [
        "También adaptado de tu script original."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "786c5deb",
      "metadata": {
        "id": "786c5deb"
      },
      "source": [
        "@title 6. Definición de Métricas y Configuración de Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca182114",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "ca182114"
      },
      "outputs": [],
      "source": [
        "# Definir Métricas de Evaluación\n",
        "print(\"\\nDefiniendo métricas de evaluación...\")\n",
        "metric = load(\"accuracy\")\n",
        "f1_metric = load(\"f1\")\n",
        "precision_metric = load(\"precision\")\n",
        "recall_metric = load(\"recall\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2262caac",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "2262caac"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = metric.compute(predictions=predictions, references=labels)\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
        "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
        "    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
        "    return {**accuracy, **f1, **precision, **recall}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c9f4d05",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "0c9f4d05"
      },
      "outputs": [],
      "source": [
        "# Configurar Argumentos de Entrenamiento\n",
        "print(\"\\nConfigurando argumentos de entrenamiento...\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=os.path.join(DRIVE_BASE_PATH, \"results\"), # Directorio para guardar los resultados en Drive\n",
        "    num_train_epochs=3,                   # Número de épocas de entrenamiento\n",
        "    per_device_train_batch_size=16,  # Tamaño del batch por dispositivo (GPU/CPU)\n",
        "    per_device_eval_batch_size=16,   # Tamaño del batch para evaluación\n",
        "    warmup_steps=500,                   # Número de pasos para el calentamiento del learning rate\n",
        "    weight_decay=0.01,                  # Regularización L2\n",
        "    logging_dir=os.path.join(DRIVE_BASE_PATH, \"logs\"), # Directorio para los logs de TensorBoard en Drive\n",
        "    logging_steps=100,\n",
        "    report_to=\"none\",                   # No reportar a ninguna plataforma (ej. wandb)\n",
        "    save_strategy=\"epoch\",            # Guardar el modelo al final de cada época\n",
        "    load_best_model_at_end=True,     # Cargar el mejor modelo al final del entrenamiento\n",
        "    metric_for_best_model=\"f1\",      # Métrica para determinar el mejor modelo\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0628c3c",
      "metadata": {
        "id": "c0628c3c"
      },
      "source": [
        "Bloque 7: Entrenamiento del Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b139a499",
      "metadata": {
        "id": "b139a499"
      },
      "source": [
        "@title 7. Entrenamiento del Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34c5c588",
      "metadata": {
        "id": "34c5c588"
      },
      "outputs": [],
      "source": [
        "# Crear el Trainer\n",
        "print(\"\\nCreando el Trainer...\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e7c5683",
      "metadata": {
        "id": "4e7c5683"
      },
      "outputs": [],
      "source": [
        "# Entrenar el Modelo\n",
        "print(\"\\nIniciando el entrenamiento del modelo...\")\n",
        "trainer.train()\n",
        "print(\"\\nEntrenamiento completado.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c14a56a",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "6c14a56a"
      },
      "outputs": [],
      "source": [
        "# Evaluar el Modelo Final\n",
        "print(\"\\nEvaluando el modelo final en el conjunto de prueba...\")\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Resultados de la evaluación final: {eval_results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ffa3529",
      "metadata": {
        "id": "5ffa3529"
      },
      "source": [
        "Bloque 8: Guardado del Modelo y División para GitHub"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05a33a6b",
      "metadata": {
        "id": "05a33a6b"
      },
      "source": [
        "Este bloque guardará el modelo en Google Drive y luego implementará la lógica para dividir el archivo pytorch_model.bin si es\n",
        "demasiado grande."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99606a6b",
      "metadata": {
        "id": "99606a6b"
      },
      "source": [
        "@title 8. Guardado del Modelo y División para GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca8bfa2a",
      "metadata": {
        "id": "ca8bfa2a"
      },
      "outputs": [],
      "source": [
        "# 10. Guardar el Modelo en Google Drive\n",
        "model_save_path_drive = os.path.join(DRIVE_BASE_PATH, \"fine_tuned_sentiment_model_full_data\")\n",
        "print(f\"\\nGuardando el modelo y tokenizador en Google Drive: {model_save_path_drive}\")\n",
        "trainer.save_model(model_save_path_drive)\n",
        "tokenizer.save_pretrained(model_save_path_drive)\n",
        "print(\"Modelo y tokenizador guardados exitosamente en Google Drive.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c77c73a",
      "metadata": {
        "id": "0c77c73a"
      },
      "source": [
        "--- Lógica para dividir el modelo para GitHub ---\n",
        "GitHub tiene un límite de 100MB por archivo.\n",
        "El archivo principal del modelo suele ser 'pytorch_model.bin' o 'model.safetensors'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4d367c5",
      "metadata": {
        "id": "e4d367c5"
      },
      "outputs": [],
      "source": [
        "model_bin_path = os.path.join(model_save_path_drive, \"pytorch_model.bin\")\n",
        "if not os.path.exists(model_bin_path):\n",
        "    # Si no es pytorch_model.bin, podría ser safetensors\n",
        "    model_bin_path = os.path.join(model_save_path_drive, \"model.safetensors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a7586b0",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "8a7586b0"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(model_bin_path):\n",
        "    file_size_mb = os.path.getsize(model_bin_path) / (1024 * 1024)\n",
        "    print(f\"\\nTamaño del archivo del modelo ({os.path.basename(model_bin_path)}): {file_size_mb:.2f} MB\")\n",
        "\n",
        "    if file_size_mb > 90: # Usamos 90MB como umbral para estar seguros por debajo de 100MB\n",
        "        print(f\"El archivo del modelo ({os.path.basename(model_bin_path)}) excede los 90MB. Dividiendo...\")\n",
        "\n",
        "        def split_file(filepath, chunk_size_mb=90):\n",
        "            chunk_size = int(chunk_size_mb * 1024 * 1024)\n",
        "            base_filename = os.path.basename(filepath)\n",
        "            output_dir = os.path.dirname(filepath)\n",
        "\n",
        "            with open(filepath, 'rb') as f:\n",
        "                part_num = 0\n",
        "                while True:\n",
        "                    chunk = f.read(chunk_size)\n",
        "                    if not chunk:\n",
        "                        break\n",
        "                    part_filename = os.path.join(output_dir, f\"{base_filename}.part{part_num:03d}\")\n",
        "                    with open(part_filename, 'wb') as part_f:\n",
        "                        part_f.write(chunk)\n",
        "                    print(f\"  Creada parte: {os.path.basename(part_filename)}\")\n",
        "                    part_num += 1\n",
        "            print(f\"Archivo '{base_filename}' dividido en {part_num} partes en {output_dir}.\")\n",
        "            print(\"Puedes subir estas partes a GitHub. Recuerda NO subir el archivo original grande.\")\n",
        "            print(\"Para reconstruir, usa el comando 'cat' o la función 'join_files' proporcionada.\")\n",
        "\n",
        "        split_file(model_bin_path)\n",
        "    else:\n",
        "        print(\"El archivo del modelo es menor de 90MB. No es necesario dividirlo para GitHub.\")\n",
        "else:\n",
        "    print(f\"Advertencia: No se encontró el archivo principal del modelo ({os.path.basename(model_bin_path)}).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25008b14",
      "metadata": {
        "id": "25008b14"
      },
      "source": [
        "Instrucciones para el Usuario"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "215d3987",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "215d3987"
      },
      "source": [
        "1. Crea un nuevo Notebook en Google Colab:\n",
        "    * Ve a Google Colab (https://colab.research.google.com/).\n",
        "    * Haz clic en Archivo -> Nuevo notebook.\n",
        "2. Copia y Pega los Bloques de Código:\n",
        "    * Copia cada bloque de código que te he proporcionado y pégalo en celdas separadas en tu nuevo notebook.\n",
        "    * Ejecuta cada celda en orden.\n",
        "3. Asegúrate de que la carpeta `SLM_Training_Results` exista en tu Google Drive antes de ejecutar el notebook, o cámbiala a una\n",
        "ruta que prefieras.\n",
        "4. Conectar Colab con GitHub (para commits):\n",
        "    * Una vez que el entrenamiento haya terminado y tengas el notebook con los resultados, puedes guardarlo en GitHub.\n",
        "    * Ve a Archivo -> Guardar una copia en GitHub....\n",
        "    * Sigue las instrucciones para autorizar Colab y seleccionar tu repositorio.\n",
        "    * Para los archivos del modelo divididos, tendrás que descargarlos de Google Drive a tu máquina local y luego subirlos\n",
        "    manualmente a GitHub (o usar git directamente en Colab si clonas tu repositorio y trabajas dentro de él, lo cual es más\n",
        "    avanzado)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ee2f956",
      "metadata": {
        "id": "2ee2f956"
      },
      "source": [
        "Función para Reconstruir el Modelo (para uso local)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52448e3e",
      "metadata": {
        "id": "52448e3e"
      },
      "source": [
        "Si necesitas reconstruir el modelo a partir de las partes descargadas de GitHub en tu máquina local, usa esta función Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "331afa2b",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "331afa2b"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7894958e",
      "metadata": {
        "id": "7894958e"
      },
      "outputs": [],
      "source": [
        "def join_files(output_filepath, part_prefix):\n",
        "    \"\"\"\n",
        "    Reconstruye un archivo a partir de sus partes.\n",
        "    output_filepath: Ruta completa del archivo final a reconstruir.\n",
        "    part_prefix: Prefijo de los archivos de las partes (ej.\n",
        "    \"./fine_tuned_sentiment_model_full_data/pytorch_model.bin.part\").\n",
        "                 Asegúrate de que incluya la ruta completa a las partes.\n",
        "    \"\"\"\n",
        "    print(f\"Reconstruyendo archivo en: {output_filepath}\")\n",
        "    with open(output_filepath, 'wb') as outfile:\n",
        "        part_num = 0\n",
        "        while True:\n",
        "            # Formato de nombre de parte: .part000, .part001, etc.\n",
        "            part_filename = f\"{part_prefix}{part_num:03d}\"\n",
        "            if not os.path.exists(part_filename):\n",
        "                break\n",
        "            print(f\"  Añadiendo parte: {os.path.basename(part_filename)}\")\n",
        "            with open(part_filename, 'rb') as infile:\n",
        "                outfile.write(infile.read())\n",
        "            part_num += 1\n",
        "    if part_num > 0:\n",
        "        print(f\"Archivo reconstruido exitosamente en '{output_filepath}' a partir de {part_num} partes.\")\n",
        "    else:\n",
        "        print(f\"Advertencia: No se encontraron partes con el prefijo '{part_prefix}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f44943b1",
      "metadata": {
        "id": "f44943b1"
      },
      "source": [
        "Ejemplo de uso (ajusta las rutas según donde descargues las partes):\n",
        "model_dir = \"./fine_tuned_sentiment_model_full_data\" # Carpeta donde están las partes\n",
        "output_file = os.path.join(model_dir, \"pytorch_model.bin\")\n",
        "part_base_name = \"pytorch_model.bin.part\" # Nombre base del archivo original\n",
        "join_files(output_file, os.path.join(model_dir, part_base_name))"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}